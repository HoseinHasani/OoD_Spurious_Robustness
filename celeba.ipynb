{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4eblo-8uaxw"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmbaClx_uaxz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CelebA\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDpmokV4uax0"
      },
      "outputs": [],
      "source": [
        "# set seed\n",
        "seed = 8\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIotE6OJuax1"
      },
      "source": [
        "## CelebA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qjr5HVK1u7pO"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "# !kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2cTDyERwa8p",
        "outputId": "c861aa55-f5e8-406a-95df-52a6b8428300"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading celeba-dataset.zip to /content\n",
            "100% 1.33G/1.33G [00:17<00:00, 129MB/s]\n",
            "100% 1.33G/1.33G [00:17<00:00, 79.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d jessicali9530/celeba-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktbdSO1Auax1",
        "outputId": "17456b2f-c1cf-449f-ed6b-b72d3e49905e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "S3t6y0k2xWWv"
      },
      "outputs": [],
      "source": [
        "# !cp ./celeba-dataset.zip ./drive/MyDrive/\n",
        "!cp ./drive/MyDrive/celeba-dataset.zip ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmzYMTZ6xmxe"
      },
      "outputs": [],
      "source": [
        "!unzip celeba-dataset.zip -d celeba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lNOIOmlkuax1"
      },
      "outputs": [],
      "source": [
        "class CelebAAttrDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transform=None, target_trasform=None, split='train'):\n",
        "        root_path = Path(root)\n",
        "        self.images_path = root_path / 'img_align_celeba/img_align_celeba'\n",
        "        self.attr_path = root_path / 'list_attr_celeba.csv'\n",
        "        self.eval_partition_path = root_path / 'list_eval_partition.csv'\n",
        "        self.transform = transform\n",
        "        self.attr = self._load_attr()\n",
        "        eval_partition = self._load_eval_partition()\n",
        "        if split == 'all':\n",
        "            partition = self.attr\n",
        "        elif split == 'train':\n",
        "            partition = eval_partition[eval_partition['partition'] == 0]\n",
        "        elif split == 'val':\n",
        "            partition = eval_partition[eval_partition['partition'] == 1]\n",
        "        elif split == 'test':\n",
        "            partition = eval_partition[eval_partition['partition'] == 2]\n",
        "        else:\n",
        "            raise ValueError('Invalid split value')\n",
        "\n",
        "        self._set_partition(partition.index)\n",
        "        self.idx2attr = {idx: attr for idx, attr in enumerate(self.attr.columns[1:])}\n",
        "        self.attr2idx = {attr: idx for idx, attr in self.idx2attr.items()}\n",
        "\n",
        "    def _set_partition(self, index):\n",
        "        self.attr = self.attr.loc[index]\n",
        "\n",
        "    def _set_attr(self, attr):\n",
        "        self.attr = attr.copy()\n",
        "\n",
        "    def _load_attr(self):\n",
        "        return pd.read_csv(self.attr_path)\n",
        "\n",
        "    def _load_eval_partition(self):\n",
        "        return pd.read_csv(self.eval_partition_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.attr)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.attr.iloc[idx, 0]\n",
        "        img_path = self.images_path / img_name\n",
        "        img = Image.open(img_path)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        attr = self.attr.iloc[idx, 1:].values.astype(np.float32)\n",
        "        return img, attr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4ilsN2Quuax2"
      },
      "outputs": [],
      "source": [
        "group_names = ['c11', 'c12', 'c21', 'c22', 'o1', 'o2']\n",
        "\n",
        "result_path = 'results/DINO_pics/'\n",
        "os.makedirs(result_path, exist_ok=True)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "        ])\n",
        "\n",
        "celeba_dataset = CelebAAttrDataset(root='./celeba', split='train', transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIIlZA_nuax2",
        "outputId": "433426b2-c5c4-4bc1-dc13-4b4e20900eae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(162770, torch.Size([3, 224, 224]))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(celeba_dataset), celeba_dataset[0][0].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ZNKlWRuax3"
      },
      "source": [
        "Sampling groups:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urtgKCCUuax4",
        "outputId": "6488d6a3-d5c8-4840-f3c9-8f6314e1b5ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['woman_black: 18784',\n",
              " 'woman_blond: 22880',\n",
              " 'woman_bald: 9',\n",
              " 'man_black: 20122',\n",
              " 'man_blond: 1387',\n",
              " 'man_bald: 3704']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = celeba_dataset.attr.copy()\n",
        "grouped_df = {}\n",
        "for gender in [-1, 1]:\n",
        "    gender_df = df[df['Male'] == gender]\n",
        "    gender_name = 'man' if gender == 1 else 'woman'\n",
        "    for hair_type in ['Black_Hair', 'Blond_Hair', 'Bald']:\n",
        "        name = f\"{gender_name}_{hair_type.split('_')[0].lower()}\"\n",
        "        hair_gender_df = gender_df[gender_df[hair_type] == 1]\n",
        "        grouped_df[name] = hair_gender_df\n",
        "\n",
        "[f'{group}: {len(group_df)}' for group, group_df in grouped_df.items()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k1As89luax4"
      },
      "source": [
        "# Dino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dellmrzXuax4",
        "outputId": "6e4912a2-90b5-46bb-de07-1af78ff39377"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vitl14_pretrain.pth\n",
            "100%|██████████| 1.13G/1.13G [00:06<00:00, 176MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DinoVisionTransformer(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
              "    (norm): Identity()\n",
              "  )\n",
              "  (blocks): ModuleList(\n",
              "    (0-23): 24 x NestedTensorBlock(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): MemEffAttention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): LayerScale()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): LayerScale()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "  (head): Identity()\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_dino = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
        "model_dino.eval()\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model_dino.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv5IZloQuax4",
        "outputId": "a5e6d745-f9de-4ee8-aa39-791bfe547b1e"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "grouped_embs = {}\n",
        "\n",
        "for group, group_df in grouped_df.items():\n",
        "    embs_list = []\n",
        "    celeba_dataset._set_attr(group_df)\n",
        "    celeba_dataloader = DataLoader(celeba_dataset, batch_size=128, shuffle=False)\n",
        "    with torch.no_grad(), tqdm.tqdm(celeba_dataloader, total=len(celeba_dataloader)) as pbar:\n",
        "        for imgs, _ in pbar:\n",
        "            imgs = imgs.to(device)\n",
        "            embs = model_dino(imgs).squeeze().cpu().numpy()\n",
        "            embs_list.append(embs)\n",
        "    embs_list = np.concatenate(embs_list)\n",
        "    grouped_embs[group] = embs_list\n",
        "\n",
        "grouped_prototypes = {group: embs.mean(axis=0, keepdims=True) for group, embs in grouped_embs.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUO4WZy11-yo",
        "outputId": "4ee9193a-0d10-436f-fb47-97668fd68e5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6, 50)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import utils\n",
        "\n",
        "euc_dist = {group: np.linalg.norm(embs - grouped_prototypes[group], axis=1) for group, embs in grouped_embs.items()}\n",
        "euc_dist = np.concatenate(list(euc_dist.values()))\n",
        "# all_embs = np.concatenate(list(grouped_embs.values()))\n",
        "# euc_dist = np.array([np.linalg.norm(embs - proto, axis=1) for group, proto in grouped_prototypes.items()])\n",
        "# euc_dist.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "TN4S5B1J5OL2"
      },
      "outputs": [],
      "source": [
        "embs_normalized = {group: embs / np.linalg.norm(embs, axis=1, keepdims=True) for group, embs in grouped_embs.items()}\n",
        "prototypes_normalized = {group: prot / np.linalg.norm(prot) for group, prot in grouped_prototypes.items()}\n",
        "cos_dist = {group: np.dot(embs, prototypes_normalized[group].T) for group, embs in embs_normalized.items()}\n",
        "cos_dist = np.concatenate(list(cos_dist.values()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "rTh1tBhq5Pjt"
      },
      "outputs": [],
      "source": [
        "embd_probs = {group: utils.softmax(dist) for group, dist in grouped_embs.items()}\n",
        "proto_probs = {group: utils.softmax(dist) for group, dist in grouped_prototypes.items()}\n",
        "cross_entropy_loss = {group: - np.dot(np.log(embd_probs[group]), proto_probs[group].T) for group in grouped_embs.keys()}\n",
        "cross_entropy_loss = np.concatenate(list(cross_entropy_loss.values()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y0YC5asc5Qgv",
        "outputId": "c0630470-c35b-465c-b71f-b33f831509c3"
      },
      "outputs": [],
      "source": [
        "result_path = 'results/DINO_pics/'\n",
        "os.makedirs(result_path, exist_ok=True)\n",
        "utils.plot_adj_mat(euc_dist, 'Unnormalized Euclidean Distance Matrix', result_path)\n",
        "utils.plot_adj_mat(cos_dist, 'Cosine Similarity Matrix', result_path)\n",
        "utils.plot_adj_mat(cross_entropy_loss, 'Cross-Entropy Matrix', result_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
